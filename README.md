# AttnBench

<div align="center">

**Benchmarking Sparse and Efficient Attention Mechanisms on Apple Silicon**

*A research project by [Dirmacs Labs](https://dirmacs.com), DIRMACS*

[![Swift](https://img.shields.io/badge/Swift-5.9+-orange.svg)](https://swift.org)
[![MLX](https://img.shields.io/badge/MLX-Swift-blue.svg)](https://github.com/ml-explore/mlx-swift)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Apple Silicon](https://img.shields.io/badge/Apple%20Silicon-M1%2FM2%2FM3%2FM4-black.svg)](https://www.apple.com/mac/)

</div>

---

## Authors

**Baalateja Kataru** Â· **Suprabhat Rapolu** Â· **Dhruv Sidhu** Â· **Shanjeth Gobinath**

---

## Overview

AttnBench is a comprehensive, statistically rigorous benchmarking framework for evaluating attention mechanisms on Apple Silicon. Implemented in Swift using [MLX](https://github.com/ml-explore/mlx-swift), it challenges common assumptions about "efficient" attention and reveals surprising performance characteristics unique to Apple's unified memory architecture.

### ğŸ”¬ What We Discovered

Our benchmarks reveal that **conventional wisdom about sparse attention doesn't apply to Apple Silicon**:

| Finding | Result | Implication |
|---------|--------|-------------|
| ğŸš« **Gather overhead is prohibitive** | 5.8â€“7.2Ã— slower than masked dense | Don't use gather-based sparse kernels |
| âœ… **Block-sparse excels at scale** | 1.9Ã— speedup at N=1024 | Use for long sequences (>512 tokens) |
| âœ… **Linear attention is viable** | 1.6Ã— speedup at N=1024 | Good for very long contexts |
| ğŸ“ˆ **Overhead grows with sequence length** | 5.7Ã— â†’ 7.3Ã— as N increases | The problem gets worse, not better |
| ğŸ¯ **No single winner** | Optimal mechanism varies by N | Use adaptive selection |

### ğŸ“Š Visual Analysis Highlights

Our seven publication-quality figures reveal striking patterns:

- **Heatmap pattern**: Gather-based SWA appears as **uniformly deep red** (0.11â€“0.15Ã— speedup) across all sequence lengthsâ€”a dramatic visual outlier
- **Crossover trajectory**: Block-sparse transitions from slower-than-MHA (red) at N=128 to nearly 2Ã— faster (bright green) at N=1024
- **L2 cache boundary**: MHA scaling shows a sharp "elbow" at N=512â†’1024, coinciding with attention matrices exceeding ~4MB L2 cache

---

## Key Results

### The Gather Overhead Problem

On Apple Silicon, "true" sparse attention using gather/slice operations is **dramatically slower** than simply computing the full attention matrix and masking it:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gather-based SWA vs Masked SWA                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  N=128:  SWA(gather) = 7.08ms  vs  MaskedSWA = 1.24ms  â”‚
â”‚          Overhead: 5.7Ã—                                 â”‚
â”‚                                                         â”‚
â”‚  N=256:  SWA(gather) = 12.35ms vs  MaskedSWA = 1.69ms  â”‚
â”‚          Overhead: 7.3Ã— (INCREASES with sequence!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why?** Apple's AMX matrix units achieve near-peak throughput for contiguous memory accesses. The overhead of gather operations (non-contiguous memory access, cache misses) exceeds any computational savings from computing fewer elements.

### Block-Sparse: The Real Winner

Block-sparse attention provides substantial speedups for long sequences:

| Sequence Length | MHA | BlockSparse (bs=32) | Speedup | p-value |
|-----------------|-----|---------------------|---------|---------|
| N=128 | 1.04 ms | 1.31 ms | 0.79Ã— | <0.01 |
| N=256 | 1.42 ms | 1.63 ms | 0.87Ã— | 0.02 |
| N=512 | 1.72 ms | 1.71 ms | **1.01Ã—** | 0.89 |
| N=1024 | 4.38 ms | 2.31 ms | **1.90Ã—** | <0.001 |

**Crossover point**: N â‰ˆ 400â€“600 (clearly visible in our figures)

### Optimal Mechanism by Sequence Length

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sequence Length    â”‚  Best Mechanism      â”‚  Latency          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  N â‰¤ 128            â”‚  GQA (kv=4)          â”‚  0.87 Â± 0.04 ms   â”‚
â”‚  128 < N â‰¤ 256      â”‚  MaskedSWA (w=128)   â”‚  1.34 Â± 0.07 ms   â”‚
â”‚  256 < N â‰¤ 512      â”‚  MQA (kv=1)          â”‚  1.59 Â± 0.08 ms   â”‚
â”‚  N > 512            â”‚  BlockSparse (bs=32) â”‚  2.31 Â± 0.12 ms   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Attention Mechanisms

### Dense Attention (O(NÂ²) complexity)

| Mechanism | Description | KV Heads | Best Use Case |
|-----------|-------------|----------|---------------|
| **MHA** | Multi-Head Attention (baseline) | h | General purpose |
| **GQA** | Grouped Query Attention | h/g | Short sequences, memory-constrained |
| **MQA** | Multi-Query Attention | 1 | KV cache reduction (87.5% savings) |

### Sparse/Efficient Attention

| Mechanism | Complexity | Description | Recommendation |
|-----------|------------|-------------|----------------|
| **SWA (Gather)** | O(NÂ·W) | True sparse via gather/slice | âŒ Avoid on Apple Silicon |
| **MaskedSWA** | O(NÂ²) | Dense + band mask | âœ… Use instead of gather |
| **BlockSparse** | O(NÂ·B) | Block-local + global tokens | âœ… Best for N > 512 |
| **LinearAttn** | O(NÂ·DÂ²) | Kernel trick linearization | âœ… Very long contexts |
| **CausalLinear** | O(NÂ·DÂ²) | Causal via cumsum | âš ï¸ Growing overhead with N |

---

## Statistical Methodology

AttnBench employs rigorous statistical methodology that goes beyond single-run measurements:

```
For each configuration:
  For run in 1..5:           # 5 independent runs
    Warmup: 5 iterations     # Excluded from timing
    Measure: 20 iterations   # Individual timing per iteration
    
  Total: 100 measurements per configuration
  Report: Mean Â± 95% CI (t-distribution)
  Significance: Welch's t-test
  Reproducibility: CV < 6% for all mechanisms
```

### Why This Matters

- **Thermal throttling**: Single runs are affected by CPU/GPU temperature
- **System variability**: Background processes affect timing
- **Confidence intervals**: Quantify measurement reliability
- **Significance testing**: Distinguish real effects from noise

---

## Requirements

- **macOS** 13.3+ (tested on macOS Tahoe 26.2)
- **Apple Silicon** (M1/M2/M3/M4)
- **Xcode** (required for Metal shader compilation)
- **CMake** and **Ninja** (for building)
- **Python 3.10+** (for analysis)
- **Typst** (optional, for compiling the paper)

### Hardware Tested

| Parameter | Value |
|-----------|-------|
| Hardware | Apple M4 Mac Mini |
| Memory | 16 GB Unified |
| CPU Cores | 10 (4P + 6E) |
| GPU Cores | 10 |
| OS | macOS Tahoe 26.2 |

---

## Quick Start

### 1. Install Dependencies

```bash
# Build tools
brew install cmake ninja

# Xcode Metal toolchain
sudo xcode-select -s /Applications/Xcode.app/Contents/Developer

# Python dependencies (for analysis)
pip install -r analysis/requirements.txt

# Typst (optional, for paper compilation)
brew install typst
```

### 2. Build

```bash
cmake -B build -G Ninja
cmake --build build
```

### 3. Run Benchmarks

```bash
./build/AttnBench > data/benchmark_results.csv
```

### 4. Generate Figures

```bash
python analysis/analyze_benchmarks.py \
  --input data/benchmark_results.csv \
  --output figures/
```

### 5. Compile Paper (Optional)

```bash
typst compile paper/paper.typ paper/paper.pdf
```

---

## Project Structure

```
attn-bench/
â”œâ”€â”€ Sources/
â”‚   â”œâ”€â”€ AttnBench/
â”‚   â”‚   â””â”€â”€ AttnBench.swift      # Benchmark driver
â”‚   â””â”€â”€ AttnBenchLib/
â”‚       â””â”€â”€ Attention.swift      # All attention implementations
â”œâ”€â”€ Tests/
â”‚   â””â”€â”€ AttnBenchTests/          # 33 unit tests
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ analyze_benchmarks.py    # Statistical analysis + figures
â”‚   â””â”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ data/
â”‚   â””â”€â”€ benchmark_results.csv    # Raw benchmark output
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ BLOG_POST.md             # Technical blog post
â”‚   â”œâ”€â”€ CONTRIBUTING.md          # Contribution guidelines
â”‚   â””â”€â”€ LINKEDIN_POST.md         # Social media content
â”œâ”€â”€ figures/                     # Generated visualizations
â”‚   â”œâ”€â”€ fig1_latency_scaling.pdf
â”‚   â”œâ”€â”€ fig2_gather_vs_masked.pdf
â”‚   â”œâ”€â”€ fig3_blocksparse_speedup.pdf
â”‚   â”œâ”€â”€ fig4_linear_attention.pdf
â”‚   â”œâ”€â”€ fig5_heatmap.pdf
â”‚   â”œâ”€â”€ fig6_dense_variants.pdf
â”‚   â”œâ”€â”€ fig7_scaling_analysis.pdf
â”‚   â””â”€â”€ statistical_summary.txt
â”œâ”€â”€ paper/
â”‚   â”œâ”€â”€ paper.typ                # Research paper (Typst)
â”‚   â”œâ”€â”€ paper.pdf                # Compiled paper
â”‚   â””â”€â”€ references.bib           # Bibliography
â”œâ”€â”€ CMakeLists.txt               # CMake build
â”œâ”€â”€ Package.swift                # Swift Package Manager
â””â”€â”€ LICENSE                      # MIT License
```

---

## Generated Outputs

### Figures

| Figure | Description |
|--------|-------------|
| `fig1_latency_scaling.pdf` | Latency vs. sequence length for all mechanisms |
| `fig2_gather_vs_masked.pdf` | Gather overhead analysis (the 5.8â€“7.2Ã— slowdown) |
| `fig3_blocksparse_speedup.pdf` | Block-sparse crossover analysis |
| `fig4_linear_attention.pdf` | Linear vs quadratic scaling comparison |
| `fig5_heatmap.pdf` | Performance heatmap (mechanisms Ã— sequence lengths) |
| `fig6_dense_variants.pdf` | MHA vs GQA vs MQA comparison |
| `fig7_scaling_analysis.pdf` | Log-log complexity analysis |

### Data Files

| File | Description |
|------|-------------|
| `statistical_summary.txt` | Full statistical report with key findings |
| `table_results.tex` | LaTeX table for papers |
| `benchmark_stats.json` | Machine-readable statistics |

---

## Practical Recommendations

### For Inference on Apple Silicon

| Scenario | Recommendation |
|----------|----------------|
| Short contexts (N â‰¤ 128) | Use **GQA** with 4 KV heads for 20% speedup |
| Medium contexts (128 < N â‰¤ 512) | Standard **MHA** or **MQA**; differences are marginal |
| Long contexts (N > 512) | Use **BlockSparse** (bs=32) for up to 90% speedup |
| Very long contexts (N >> 1024) | Consider **LinearAttn** if approximation is acceptable |
| Sliding window needed | **Always use masked implementation**, never gather-based |
| Memory-constrained | Use **MQA** for 87.5% KV cache reduction |

### For Framework Developers

1. **Default to masked-dense implementations** â€” Gather-based sparse kernels should be opt-in, not default
2. **Implement adaptive dispatch** â€” Select mechanism based on sequence length at runtime
3. **Tune block sizes per chip** â€” Our bs=32 > bs=64 finding may differ on M1/M2/M3
4. **Profile cumsum operations** â€” Causal linear attention overhead grows with sequence length

### For Model Architects

1. **GQA over MQA for short-context applications** â€” GQA provides consistent speedups; MQA has short-sequence overhead
2. **Consider hybrid architectures** â€” Dense attention in early layers, sparse in later layers
3. **Window size is free in masked SWA** â€” Choose based on model quality, not performance

---

## Running Tests

```bash
xcodebuild test -scheme AttnBench-Package -destination 'platform=OS X'
```

All 33 tests verify:
- Shape correctness for all mechanisms
- Finite outputs (no NaN/Inf)
- Mechanism naming conventions
- Statistical utility functions

---

## Citation

If you use AttnBench in your research, please cite:

```bibtex
@misc{attnbench2024,
  title={AttnBench: Benchmarking Sparse and Efficient Attention 
         Mechanisms on Apple Silicon},
  author={Kataru, Baalateja and Rapolu, Suprabhat and 
          Sidhu, Dhruv and Gobinath, Shanjeth},
  year={2024},
  institution={Dirmacs Labs, DIRMACS},
  howpublished={\url{https://github.com/dirmacs/attn-bench}}
}
```

---

## References

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Vaswani et al., 2017
- [FlashAttention](https://arxiv.org/abs/2205.14135) â€” Dao et al., 2022
- [Longformer](https://arxiv.org/abs/2004.05150) â€” Beltagy et al., 2020
- [BigBird](https://arxiv.org/abs/2007.14062) â€” Zaheer et al., 2020
- [Linear Transformers](https://arxiv.org/abs/2006.16236) â€” Katharopoulos et al., 2020
- [GQA](https://arxiv.org/abs/2305.13245) â€” Ainslie et al., 2023
- [MLX](https://github.com/ml-explore/mlx) â€” Apple ML Research

---

## License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## Acknowledgments

- [MLX Swift](https://github.com/ml-explore/mlx-swift) â€” Apple's ML framework for Apple Silicon
- **Dirmacs Labs** for supporting this research initiative

---

<div align="center">

**[Dirmacs Labs](https://dirmacs.com)** â€” Exploring cutting-edge technologies from hardware to ML systems

</div>