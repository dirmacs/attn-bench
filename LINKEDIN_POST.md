# LinkedIn Post for AttnBench Research

## Short Version (for standard posts)

---

ğŸš€ **New Research from Dirmacs Labs: Your "Efficient" Attention Might Actually Be Slower on Apple Silicon**

We just published AttnBenchâ€”a comprehensive benchmarking study of attention mechanisms on Apple M-series chips. The results challenge conventional wisdom:

ğŸ“Š **Key Findings:**

âŒ **Gather-based sparse attention is 5.8â€“7.2Ã— SLOWER** than dense masked attention on Apple Silicon. Yes, slowerâ€”and the overhead *increases* with sequence length.

âœ… **Block-sparse attention achieves 1.9Ã— speedup** at sequence length 1024 (p < 0.001)

âœ… **Linear attention provides 1.6Ã— speedup** for long sequences

ğŸ¯ **No single mechanism wins everywhere**â€”GQA excels at short sequences, block-sparse dominates at long ones

ğŸ’¡ **Why it matters:** With 100M+ Macs powered by Apple Silicon, optimizing ML inference for this hardware is increasingly critical. But techniques that work on NVIDIA GPUs don't always transferâ€”the unified memory architecture fundamentally changes the game.

ğŸ”¬ Rigorous methodology: 5 runs Ã— 20 iterations = 100 measurements per config, 95% CIs, Welch's t-tests

ğŸ“„ Full paper, code, and data are open-source.

#MachineLearning #AppleSilicon #MLX #TransformerModels #Attention #DeepLearning #Research #OpenSource #MLOps #AI

---

## Long Version (for articles/newsletters)

---

### ğŸ”¬ New Research: Benchmarking Attention Mechanisms on Apple Silicon

**Dirmacs Labs** is excited to share our latest research: **AttnBench**, a comprehensive study of attention mechanism performance on Apple M-series processors.

#### The Motivation

Everyone talks about "efficient" attentionâ€”sliding window, sparse patterns, linear attention. But there's a problem: most benchmarks target NVIDIA GPUs. Apple Silicon has a fundamentally different architecture (unified memory, AMX matrix units, Metal compute), and we wanted to know: **do these efficiency techniques actually help?**

The answer surprised us.

---

#### ğŸ”‘ Key Findings

**1. Gather-Based Sparse Attention is Catastrophically Slow**

True sparse attention using gather/slice operations is **5.8â€“7.2Ã— slower** than simply computing the full attention matrix and masking it. Even more surprising: the overhead *increases* with sequence length.

| Sequence | Gather-based | Masked Dense | Overhead |
|----------|--------------|--------------|----------|
| N=128 | 7.08 ms | 1.24 ms | 5.7Ã— |
| N=256 | 12.35 ms | 1.69 ms | 7.3Ã— |

**Why?** Apple's AMX units achieve near-peak throughput for contiguous memory. The memory indirection from gather operations causes cache misses that exceed any computational savings.

**2. Block-Sparse Is the Real Winner**

Block-sparse attention achieves **1.9Ã— speedup** over standard MHA at N=1024 (p < 0.001). The crossover point is around N=400â€“600.

**3. No Single Mechanism Dominates**

- N â‰¤ 128: GQA wins (20% faster)
- N â‰ˆ 256â€“512: Dense variants are competitive  
- N > 512: Block-sparse dominates

This suggests production systems should implement **adaptive attention dispatch** based on input characteristics.

**4. The L2 Cache Boundary Matters**

MHA scaling shows a sharp "elbow" at N=512â†’1024, coinciding with attention matrices exceeding Apple M4's ~4MB L2 cache. Cache-aware implementations like block-sparse avoid this cliff.

---

#### ğŸ“Š Methodology

Unlike typical benchmarks that report single measurements:
- 5 independent runs per configuration
- 20 iterations per run (100 total measurements)
- 95% confidence intervals using t-distribution
- Welch's t-tests for significance
- Coefficient of variation < 6% across all mechanisms

---

#### ğŸ’» What's Included

- **Full paper** (27 pages) with detailed analysis
- **7 publication-quality figures** with visual insights
- **Swift/MLX implementation** of 8 attention variants
- **Python analysis pipeline** for statistical analysis
- **All raw data** for reproducibility

---

#### ğŸ¯ Practical Recommendations

**For Inference:**
- Use GQA for short sequences (â‰¤128 tokens)
- Use block-sparse for long sequences (>512 tokens)
- Never use gather-based sparse attention on Apple Silicon

**For Framework Developers:**
- Default to masked-dense implementations
- Implement adaptive dispatch based on sequence length
- Profile cumsum operationsâ€”causal linear attention has growing overhead

---

#### About Dirmacs Labs

Dirmacs Labs is the R&D division of DIRMACS, focused on exploring cutting-edge technologies from hardware-aware algorithm design to high-level ML systems. This research represents our commitment to rigorous, reproducible benchmarking that challenges assumptions and provides actionable insights.

---

**ğŸ“ Links:**
- GitHub: [github.com/dirmacs/attn-bench]
- Paper: Available in repository
- Blog Post: Technical deep-dive included

**Authors:** Baalateja Kataru, Suprabhat Rapolu, Dhruv Sidhu, Shanjeth Gobinath

#MachineLearning #AppleSilicon #MLX #Transformers #AttentionMechanism #DeepLearning #Research #OpenSource #Benchmarking #AI #NLP #LLM

---

## Tweet Thread Version

---

ğŸ§µ 1/8: We just published AttnBenchâ€”a deep dive into attention mechanism performance on Apple Silicon.

The TL;DR will surprise you: "efficient" sparse attention can be 7Ã— SLOWER than dense attention on M-series chips.

Here's what we found ğŸ‘‡

2/8: ğŸš« FINDING #1: Gather-based sparse attention is catastrophically slow

Sliding window attention using gather/slice: 12.35ms
Same window using dense + mask: 1.69ms

That's 7.3Ã— overhead. And it gets WORSE at longer sequences.

3/8: âœ… FINDING #2: Block-sparse is the real winner

At sequence length 1024:
- Standard MHA: 4.38ms
- Block-sparse: 2.31ms
- Speedup: 1.9Ã— (p < 0.001)

The crossover point is around N=500.

4/8: ğŸ¯ FINDING #3: No single mechanism wins everywhere

- Nâ‰¤128: GQA wins (20% faster)
- Nâ‰ˆ512: Dense variants tie
- N>512: Block-sparse dominates

Production systems should use adaptive dispatch.

5/8: ğŸ§  WHY does this happen?

Apple's AMX units are incredibly fast at dense matmul with contiguous memory. Gather operations cause cache misses that exceed any compute savings.

The unified memory architecture changes everything.

6/8: ğŸ“Š Our methodology:
- 5 runs Ã— 20 iterations = 100 measurements per config
- 95% confidence intervals
- Welch's t-tests for significance
- CV < 6% for all mechanisms

This isn't a quick benchmarkâ€”it's rigorous science.

7/8: ğŸ’¡ PRACTICAL TAKEAWAYS:

âœ… Use masked implementations, not gather-based
âœ… Switch to block-sparse for sequences >512
âœ… Consider adaptive mechanism selection
âŒ Avoid causal linear attention (3Ã— overhead at N=512)

8/8: ğŸ“„ Everything is open-source:
- Full paper (27 pages)
- 7 publication figures
- Swift/MLX code
- Python analysis pipeline

Link in bio.

Research by @DirmacsLabs ğŸ”¬

#MachineLearning #AppleSilicon #OpenSource